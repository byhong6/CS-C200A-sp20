{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Linear Regression\n",
    "\n",
    "In this lecture we will introduce **linear models** and **least squares linear regression**.  We will explain the basic *parametric model* formulation and how to optimize it using **linear algebra** and **geometry** rather than gradient descent.  \n",
    "\n",
    "**Updated** This notebook has been slightly updated with additional explanations from the original posting.  I have also added a video walk-through of the notebook which is imported at the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "\n",
    "In this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support embedding YouTube Videos in Notebooks\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Lecture Format\n",
    "\n",
    "This is the first of a series of lectures that are designed to be completed online rather than in class.  This new lecture format is experimental and we ask for your feedback on Piazza. \n",
    "\n",
    "The playlist for all the videos in this lecture can be found here:\n",
    "\n",
    "[Least Squares Linear Regression](https://www.youtube.com/playlist?list=PLcK2S75CXo8MNPMb80AZOfwCFfWIdQFPV)\n",
    "\n",
    "### Intended Use\n",
    "We have designed this video notebook to be a series of short videos followed by simple questions (with answers provided) and demos. There are no grades for working through this notebook.  However, we hope that by combining videos with activities this will actually be more engaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "The following short video provides a recap of the modeling recipe (model, loss, optimize).  If you are comfortable with this you can safely skip this video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"u0YycwQ814c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap Question\n",
    "\n",
    "What are the three stages of modeling used in this class?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    In this class, the recipe for modeling consists of defining the model, defining the loss, and optimizing the model with respect to the loss.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression Setting\n",
    "\n",
    "In the following video we introduce the multiple linear regression setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"21PN6a3s22I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diamonds Dataset\n",
    "\n",
    "To motivate the multilinear regression setting we will work with the classic diamonds dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/diamonds.csv.zip\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the dataset corresponds to a single diamond.  The fields are\n",
    "\n",
    "1. **carat**: The weight of the diamonds\n",
    "2. **cut**: The quality of the cut is an *ordinal* variable with values (`Fair`, `Good`, `Very Good`, `Premium`, and `Ideal`)\n",
    "3. **color**: The color of the diamond is an *ordinal* variable with values `J` (worst) to `D` (best).\n",
    "4. **clarity**: How obvious inclusions are within the diamond. This is an *ordinal* variable with values `I1` (worstt), `SI2`, `SI1`, `VS2`, `VS1`, `VVS2`, `VVS1`, `IF` (best)\n",
    "5. **depth**: The height of a diamond, measured from the culet to the table, divided by its average girdle diameter.\n",
    "6. **table**: The width of the diamond's table expressed as a percentage of its average diameter.\n",
    "7. **price**: Price of the diamond in USD.\n",
    "8. **x**: length measured in mm\n",
    "9. **y**: width measured in mm\n",
    "10. **z**: depth measured in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in **predicting the price of a diamond given it's characteristics**.  Therefore, we would like to fit a model:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\text{Diamond's Characteristics}\\right) \\rightarrow \\text{Price}\n",
    "$$\n",
    "\n",
    "Suppose for now we focus only on the diamond's **carat**, **depth**, and **table** values.  In this case, we would want a model of the form:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\textbf{carat}, \\textbf{depth}, \\textbf{table}\\right) \\rightarrow \\textbf{price}\n",
    "$$\n",
    "\n",
    "We could express this as a linear model of the form:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\textbf{carat}, \\textbf{depth}, \\textbf{table}\\right) \n",
    "=\n",
    "\\theta_0 + \n",
    "\\theta_1 * \\textbf{carat} +\n",
    "\\theta_2 * \\textbf{depth} +\n",
    "\\theta_3 * \\textbf{table} \n",
    "$$\n",
    "\n",
    "Where $\\theta_0$ is the **intercept** parameter and $\\theta_1$, ..., $\\theta_3$ are the **\"slopes\"** with respect to each of the covariates.  \n",
    "\n",
    "Focusing on just the **carat**, **depth**, and **table** features and the price our dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['carat', 'depth', 'table', 'price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we saw that the predictions for the entire data set $\\hat{\\mathbb{Y}}$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta  \n",
    "$$\n",
    "\n",
    "The **covariate matrix** $\\mathbb{X} \\in \\mathbb{R}^{n,d+1}$ has $n$ rows corresponding to each record in the dataset and $d+1$ columns corresponding to the original $d$ columns in the data plus an additional 1s column vector.  For example, the first five rows of $\\mathbb{X}$ for this dataset would look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "      <th>2</th>\n",
    "      <th>3</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>61.5</td>\n",
    "      <td>55.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.21</td>\n",
    "      <td>59.8</td>\n",
    "      <td>61.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>56.9</td>\n",
    "      <td>65.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.29</td>\n",
    "      <td>62.4</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.31</td>\n",
    "      <td>63.3</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract our covariate matrix and add a column of all 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = data[['carat', 'depth', 'table']].shape\n",
    "print(\"n = \", n)\n",
    "print(\"d = \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function attaches 1s to each of the rows in the matrix.  Here I am using the `np.hstack` function to add an extra column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(data):\n",
    "    return np.hstack([np.ones((n,1)), data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add a 1s column to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = add_ones_column(data[['carat', 'depth', 'table']].to_numpy())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the linear model in python using numpy.  Here I will assume the input is in row vector form so this function will work with a single row or the entire matrix.  Note in lecture we discuss the case where a single row is stored in column vector form (as this is the case in most math textbooks).\n",
    "\n",
    "The `@` symbol is the matrix multiply operation and is equivalent to writing `xt.dot(theta)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(theta, xt): \n",
    "    return xt @ theta # The @ symbol is matrix multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any value of $\\theta$ (even a random one) we can make a prediction using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta_guess = np.random.randn(d+1, 1)\n",
    "theta_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Above, we used `np.random.randn`, which generates draws from a normal (Gaussian) distribution with mean 0 and variance 1 (the parameters `d+1` and `1` specify the number of rows and columns that the returning array should have). The fact that these values are drawn from a normal distribution are irrelevant here, just think of them as coming from a random number generator._\n",
    "\n",
    "Making a single prection for the 3rd row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(theta_guess, X[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction for all the rows at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(theta_guess, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the notation directly from the video lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = X @ theta_guess\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using our function produces the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = linear_model(theta_guess, X)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Minimizing the Loss\n",
    "\n",
    "The next video defines and minimizes the least squares loss by using a geometric argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"zkJ3CULN8Wk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To elaborate a little on the video above:\n",
    "\n",
    "Recall from linear algebra that two vectors are orthogonal when their dot product is zero. The standard way to write the dot product is $a \\cdot b$, but it can also be written as $a^T b$ or $b^T a$ (all of these forms evaluate to $a_1 b_1 + a_2 b_2 + ... + a_n b_n$).\n",
    "\n",
    "The notion of orthogonality extends to matrices. If we want some vector $r$ to be orthogonal to the span of a matrix $X$ (i.e. to each column in the matrix), that's the same as saying the vector is orthogonal to the entire matrix. To denote this, we can say $X \\cdot r = 0$ or $X^T \\cdot r = 0$. In our case, $r = y - X  \\hat{\\theta}$, so we have $X^T (y - X \\hat{\\theta}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Least Squares Loss\n",
    "\n",
    "The Least Squares Regression loss is the average squared loss:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\frac{1}{n}\\sum_{i=1}^n \\left( \\mathbb{Y}_i - \\left(\\mathbb{X} \\theta\\right)_i \\right)^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n \\left( \\mathbb{Y}_i - \\mathbb{X}_i \\theta \\right)^2 \\\\\n",
    "&= \\frac{1}{n} || \\mathbb{Y} - \\mathbb{X} \\theta ||_2^2 \\\\\n",
    "&= \\frac{1}{n}\\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)^T \\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)\n",
    "\\end{align}\n",
    "\n",
    "Note: Above, we used 2 facts from linear algebra. First, we used the fact that the square of the $L_2$ norm of a vector is equal to the sum of the squares of its components. Put less cryptically, that means that for any vector $r$, $||r||_2^2 = r_1^2 + r_2^2 + ... + r_n^2 = \\sum_{i = 1}^n r_i^2$. We used this going from line 2 to 3, where $r = \\mathbb{Y} - \\mathbb{X} \\theta$. The second fact we used was that (again for any vector $r$) $||r||_2^2 = r \\cdot r = r^Tr$.\n",
    "\n",
    "Here $\\mathbb{Y}_i$ is the $i$th observed response (diamond price) and $\\mathbb{X}_i$ is the **row vector** coresponding to the $i$th row of the covariates with an added 1 at the beginning. \n",
    "\n",
    "It is worth noting that since our goal is typically to minimize the loss and we often don't care about its actual value we can drop the $\\frac{1}{n}$:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)^T \\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Loss\n",
    "\n",
    "In order to calculate the loss we need the response vector $\\mathbb{Y}$ (the thing we are trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =  data[['price']].to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly writing the **average squared loss**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(theta):\n",
    "    return np.mean((Y - X @ theta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_loss(theta_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrix notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(theta):\n",
    "    return ((Y - X @ theta).T @ (Y - X @ theta)).item() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_loss(theta_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there is some variation in the last digits of the floating point calculation.  This is an issue with ordering of numerical operations over large sums.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Loss\n",
    "\n",
    "In the video lecture we defined the normal equation as:\n",
    "\n",
    "$$\n",
    "\\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n",
    "\n",
    "If we solve for $\\hat{\\theta}$ this is the minimizer of the squared loss with respect to our data.  \n",
    "\n",
    "If we assume $\\mathbb{X}^T \\mathbb{X}$ is invertible (full rank) then we get the following solution:\n",
    "\n",
    "$$\n",
    " \\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Normal Equation\n",
    "\n",
    "The following video provides a better intuition of the shape of each of the terms in the solution to the normal equation as well as how to better solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"9vWR-19WQKU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is not as numerically stable or efficient. We can compute $\\hat{\\theta}$ by direction using matrix inversion.  To do this, we import the `inv` function for the numpy linear algebra library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = inv(X.T @ X) @ X.T @ Y\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to solve the normal equations is using the `solve` function to solve the linear systems:\n",
    "\n",
    "$$\n",
    "A \\theta = b\n",
    "$$\n",
    "\n",
    "where $A=\\mathbb{X}^T \\mathbb{X}$ and $b=\\mathbb{X}^T \\mathbb{Y}$.  The `solve` function can be implement slightly more efficiently by avoiding the inversion and subsequent matrix multiply.  For matrices which are less well behaved, solve can also be more numerically stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = solve(X.T @ X, X.T @ Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just computed the global optimum of the squared loss.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we have estimated the model parameters $\\hat{\\theta}$ we can now also predict the price $\\hat{\\mathbb{Y}}$ for each of our diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = X @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing the Model\n",
    "\n",
    "In previous lectures we have plotted the residual.  We can do that for each of the dimensions but in general it will be difficult to visualize the residual directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual vs Carat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,1], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Carat\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we see that we are over estimating the price of big diamonds and under estimating the price of small diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual vs Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,2], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Depth\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no consistent pattern in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals vs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,3], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Table\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting $\\hat{\\mathbb{Y}}$ vs $\\mathbb{Y}$\n",
    "A better way to visualize higher dimensional regression models is to compare the Predicted Value against the Observed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = Y.flatten(), y = Y_hat.flatten(), opacity=0.2)\n",
    "fig.add_trace(go.Scatter(x=[0,20000], y=[0, 20000], name=\"Y_hat=Y\"))\n",
    "fig.update_xaxes(title=\"Y\")\n",
    "fig.update_yaxes(title=\"Y_hat\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see?  We are under estimating expensive diamonds!  Perhaps we should consider the cut, color, and clarity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Walk-Through\n",
    "\n",
    "Several of you asked for a video walk-through of the notebook.  I have created one here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"7M3yLv_DYX4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats Next\n",
    "\n",
    "In the next notebook lecture we will see how to improve this model by applying feature engineering techniques to incorporate categorical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
