{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='solar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Synthetic Data\n",
    "\n",
    "For this lecture we use a simple synthetic dataset to simplify the presentation of ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "np.random.seed(42)\n",
    "noise = 0.7\n",
    "w_true = np.array([1., 3.])\n",
    "quad = -4\n",
    "\n",
    "x = np.sort(np.random.rand(n)*2 - 1.)\n",
    "y = w_true[0] + w_true[1] * x + quad*(x**2) + noise * np.random.randn(n)\n",
    "x[0] = -1.5\n",
    "y[0] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_plot = go.Scatter(x=x, y=y, name=\"Raw Data\", mode='markers')\n",
    "fig = go.Figure([raw_data_plot])\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining The Model and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, x):\n",
    "    return w[0] + w[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous lecture we showed how to analytically derive the minimizer for the squared loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    return np.mean (standard_units(x) * standard_units(y))\n",
    "\n",
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y)*np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the weights based on the functions from prior lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse = np.array([intercept(x,y), slope(x,y)])\n",
    "w_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(w_mse, x)\n",
    "analytic_mse_line = go.Scatter(x=x,y=y_hat, name=\"Analytic MSE\")\n",
    "fig = go.Figure([raw_data_plot, analytic_mse_line])\n",
    "fig.update_layout(height=700)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for Plotly only to generate residual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_lines(x, y, yhat):\n",
    "    return [ \n",
    "        go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "        for (x, y, yhat) in zip(x, y, y_hat)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y =   y -y_hat , mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Loss Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(yhat, y):\n",
    "    return ((yhat - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustively try a large number of possible parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Range of intercpet values\n",
    "w0values = np.linspace(w_mse[0]-1, w_mse[0]+1, 50) \n",
    "# Compute Range of Slope values\n",
    "w1values = np.linspace(w_mse[1]-1, w_mse[1]+1, 50)\n",
    "# Construct \"outer product of all possible values\"\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "# Convert into a tall matrix with each row corresponding to a possible parameterization\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "# Compute the Loss for each parameterization\n",
    "mse_loss_values = np.array([mse_loss(y, model(w, x)) for w in ws]).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a really cool looking visualization of the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    specs=[[{'type': 'contour'}, {'type': 'surface'}]])\n",
    "# Make Contour Plot and Point\n",
    "fig.add_trace(go.Contour(x=w0values, y=w1values, z=mse_loss_values , colorbar=dict(x=-.1)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[w_mse[0]], y=[w_mse[1]]), row=1, col=1)\n",
    "# Make Surface Plot and Point\n",
    "fig.add_trace(go.Surface(x=w0values, y=w1values, z=mse_loss_values, opacity=0.9), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=[w_mse[0]], y=[w_mse[1]], z=[mse_loss(y, model(w_mse,x))]), row=1, col=2)\n",
    "# Cleanup Legend\n",
    "fig.update_layout(scene=dict(xaxis=dict(title='Slope'), yaxis=dict(title='Intercept'), zaxis=dict(title=\"MSE Loss\")))\n",
    "fig.update_xaxes(title_text=\"Intercept\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Slope\", row=1, col=1)\n",
    "fig.update_layout(height=700)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the $L^1$ Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just solved the model for the $L^2$ loss.  We now examine the $L^1$ loss.  We first begin by visualizing the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(yhat, y):\n",
    "    return (np.abs(yhat - y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Range of intercpet values\n",
    "w0values = np.linspace(w_mse[0]-1, w_mse[0]+1, 50) \n",
    "# Compute Range of Slope values\n",
    "w1values = np.linspace(w_mse[1]-1, w_mse[1]+1, 50)\n",
    "# Construct \"outer product of all possible values\"\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "# Convert into a tall matrix with each row corresponding to a possible parameterization\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "# Compute the Loss for each parameterization\n",
    "abs_loss_values = np.array([abs_loss(y, model(w, x)) for w in ws]).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    specs=[[{'type': 'contour'}, {'type': 'surface'}]])\n",
    "# Make Contour Plot and Point\n",
    "fig.add_trace(go.Contour(x=w0values, y=w1values, z=abs_loss_values, colorbar=dict(x=-.1)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[w_mse[0]], y=[w_mse[1]]), row=1, col=1)\n",
    "# Make Surface Plot and Point\n",
    "fig.add_trace(go.Surface(x=w0values, y=w1values, z=abs_loss_values, opacity=0.9), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=[w_mse[0]], y=[w_mse[1]], z=[abs_loss(y, model(w_mse,x))]), row=1, col=2)\n",
    "# Cleanup Legend\n",
    "fig.update_layout(scene=dict(xaxis=dict(title='Slope'), yaxis=dict(title='Intercept'), zaxis=dict(title=\"MSE Loss\")))\n",
    "fig.update_xaxes(title_text=\"Intercept\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Slope\", row=1, col=1)\n",
    "fig.update_layout(height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([raw_data_plot, analytic_mse_line])\n",
    "fig.update_layout(height=700)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Introduction to Algorithmic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we are going to introduce PyTorch.  PyTorch is sort of like learning how to use Thor's hammer, it is way overkill for basically everything you will do and is probably the wrong solution to most problems you will encounter. However, it also really powerful and will give you the skills needed to take on very challenging problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a variable $\\theta$ with an initial value 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor([1.0], requires_grad=True, dtype=torch.float64)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we compute the following value from our tensor `theta`\n",
    "\n",
    "$$\n",
    "z = \\left(1 - log\\left(1 + \\exp(\\theta) \\right) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (1 - torch.log(1 + torch.exp(theta)))**2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every derived value has an attached a gradient function that is used to compute the backwards path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz\n",
    "# !brew install graphviz\n",
    "# from torchviz import make_dot\n",
    "# make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were unable to run the above cell here is what the output looks like:\n",
    "\n",
    "<img src=\"torch_graph.png\" alt=\"TorchGraph\" style=\"height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These backward functions tell Torch how to compute the gradient via the chain rule.  This is done by invoking backward on the computed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `item` to extract a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this witht he hand computed derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial\\theta} &= \\frac{\\partial}{\\partial\\theta}\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)^2 \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\frac{\\partial}{\\partial\\theta} \\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right) (-1) \\frac{\\partial}{\\partial\\theta} \\log\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\frac{\\partial}{\\partial\\theta}\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\exp(\\theta) \\\\\n",
    "  & = -2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_derivative(theta):\n",
    "    return -2 * (1 - np.log(1 + np.exp(theta))) * np.exp(theta) / (1. + np.exp(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_derivative(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the Absolute Loss Using Gradient Descent\n",
    "\n",
    "Here we will use pytorch to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy data to tensors.  I am storing these tensors in a dictionary data so I don't confuse them with my other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    x = torch.from_numpy(x),\n",
    "    y = torch.from_numpy(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Model \n",
    "This is the most basic way to define a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel:\n",
    "    def __init__(self):\n",
    "        self.w = torch.zeros(2, 1, requires_grad=True)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        w = self.w\n",
    "        return w[0] + w[1] * x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = SimpleLinearModel()\n",
    "lin_model(data['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Loss\n",
    "\n",
    "There are many built in loss functions but we will build our own to see how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss_torch(ypred, y):\n",
    "    return torch.abs(ypred - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = abs_loss_torch(lin_model(data['x']), data['y'])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements a basic version of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model, loss_fn, x, y, lr=1., nsteps=100):\n",
    "    values = [model.w.data.numpy().flatten()] # Track parameter values for later viz.\n",
    "    for i in range(nsteps):\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        # We compute the update in a torch.no_grad context to prevent torch from \n",
    "        # trying to compute the gradient of the gradient calculation.\n",
    "        with torch.no_grad():\n",
    "            model.w -= lr * model.w.grad\n",
    "            # We also need to clear the gradient buffer otherwise future calls will\n",
    "            # accumulate the gradient. \n",
    "            model.w.grad.zero_()\n",
    "            # print(i, loss.item())\n",
    "            values.append(model.w.data.numpy().flatten())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = SimpleLinearModel()\n",
    "values = gradient_descent(lin_model, abs_loss_torch, data['x'], data['y'], nsteps=50)\n",
    "print(abs_loss_torch(data['y'], lin_model(data['x'])))\n",
    "w_abs = lin_model.w.data.numpy().flatten()\n",
    "print(w_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0values = np.linspace(-1., 1, 50)\n",
    "w1values = np.linspace(-.5, 5, 50)\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "loss = np.array([abs_loss(y, model(w, x)).item() for w in ws]).reshape(u.shape)\n",
    "fig = go.Figure([go.Contour(x=w0values, y=w1values, z=loss, colorbar=dict(x=-.1)),\n",
    "          go.Scatter(x=[w_mse[0]], y=[w_mse[1]], name=\"MSE\", mode=\"markers\"),\n",
    "          go.Scatter(x=[w_abs[0]], y=[w_abs[1]], name=\"Abs\", mode=\"markers\"),\n",
    "          go.Scatter(x=values[:,0], y=values[:,1], name=\"Path\", mode=\"markers+lines\", \n",
    "                     line=go.scatter.Line(color='white'))])\n",
    "fig.update_layout(height=600, xaxis_title=\"Intercept\", yaxis_title=\"Slope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the current best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(w_abs, x)\n",
    "gd_abs_line = go.Scatter(x=x,y=y_hat, name=\"GD ABS\")\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line, gd_abs_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y = y - y_hat, mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving The Model\n",
    "\n",
    "In the above we notice some curvature in the residual plot and decide to build a more complex model with an extra quadratic term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticModel:\n",
    "    def __init__(self):\n",
    "        self.w = torch.zeros(3, 1, requires_grad=True)\n",
    "    def predict(self, x):\n",
    "        w = self.w\n",
    "        return w[0] + w[1] * x + w[2] * (x * x)\n",
    "    def __call__(self, x):\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model = QuadraticModel()\n",
    "values = gradient_descent(quad_model, abs_loss_torch, data['x'], data['y'], nsteps=1000)\n",
    "print(abs_loss_torch(data['y'], quad_model(data['x'])).item())\n",
    "w_quad_abs = quad_model.w.data.numpy().flatten()\n",
    "print(w_quad_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = quad_model(data['x']).numpy().flatten()\n",
    "\n",
    "gd_quad_abs_line = go.Scatter(x=x, y=y_hat, name=\"GD Quad ABS\")\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line, gd_abs_line, gd_quad_abs_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y = y - y_hat , mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a mechanism to sample the data.  Since this is central to SGD (and therefore PyTorch) there is a built in way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(data['x'], data['y'])\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dict(x=x, y=y) for x, y in loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic SGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(model, loss_fn, dataset, lr=1., nepochs=100, batch_size=10):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    values = [model.w.data.numpy().flatten()] # Track parameter values for later viz.\n",
    "    for i in range(nepochs):\n",
    "        for (x, y) in loader:\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            # We compute the update in a torch.no_grad context to prevent torch from \n",
    "            # trying to compute the gradient of the gradient calculation.\n",
    "            with torch.no_grad():\n",
    "                model.w -= lr/(i+1) * model.w.grad\n",
    "                # We also need to clear the gradient buffer otherwise future calls will\n",
    "                # accumulate the gradient. \n",
    "                model.w.grad.zero_()\n",
    "                # print(i, loss.item())\n",
    "                values.append(model.w.data.numpy().flatten())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model_sgd = QuadraticModel()\n",
    "values = stochastic_gradient_descent(quad_model_sgd, abs_loss_torch, dataset, \n",
    "                                     lr=1.0, nepochs=20, batch_size=10)\n",
    "print(abs_loss_torch(data['y'], quad_model_sgd(data['x'])).item())\n",
    "w_quad_abs_sgd = quad_model_sgd.w.data.numpy().flatten()\n",
    "print(w_quad_abs_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter3d(x=values[:,0], y=values[:,1], z=values[:,2], \n",
    "                              marker=dict(color=np.linspace(0,1,values.shape[0]))\n",
    "                             )])\n",
    "fig.update_layout(height = 800)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
