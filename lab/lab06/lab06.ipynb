{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab06.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be working with the diamond dataset from Lecture 15. You will fit a linear model to predict the price of a diamond using its characteristics. You will get experience with extracting and creating features  using techniques such as one-hot encoding or log transformation to improve the accuracy of your model. At the end, you will get a chance to create your own features for the linear model! \n",
    "\n",
    "**This assignment should be completed and submitted by 11:59 PM on Monday March 30, 2020.**\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*List collaborators here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the diamond dataset from Lecture 15 and look at the fields in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/diamonds.csv.zip\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the dataset corresponds to a single diamond.  The fields are\n",
    "\n",
    "1. **carat**: The weight of the diamonds.\n",
    "2. **cut**: The quality of the cut. This is an *ordinal* variable which takes on a value in the set: {`Fair`, `Good`, `Very Good`, `Premium`, and `Ideal`}.\n",
    "3. **color**: The color of the diamond. This is an *ordinal* variable which takes on a value from the set of characters between `J` (worst) and `D` (best).\n",
    "4. **clarity**: How obvious inclusions are within the diamond. This is an *ordinal* variable that takes on a value from the set: {`I1` (worst), `SI2`, `SI1`, `VS2`, `VS1`, `VVS2`, `VVS1`, `IF` (best)}.\n",
    "5. **depth**: The height of a diamond, measured from the culet to the table, divided by its average girdle diameter.\n",
    "6. **table**: The width of the diamond's table expressed as a percentage of its average diameter.\n",
    "7. **price**: Price of the diamond in USD.\n",
    "8. **x**: Length of the diamond measured in mm.\n",
    "9. **y**: Width of the diamond measured in mm.\n",
    "10. **z**: Depth of the diamond measured in mm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in **predicting the price of a diamond given it's characteristics**.  Mathematically, we would like to fit a linear model with parameters $\\theta$ corresponding to features $\\textbf{x}$ to best capture the price of the diamonds:\n",
    "\n",
    "$$\n",
    "f_{\\theta} (\\textbf{x}) \\rightarrow \\text{Price}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Note that in this lab, we will look at how incorporating different sets of features affect the loss on the entire dataset. However, as you will see in the next few lectures, this is by no means the best way to evaluate different sets of features. In future lectures, we will introduce the concept of training set, test set, validation set, and cross-validation to help us rigorously and correctly evaluate how good a set of features is for the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of the lab, we will be focusing on diamond's **carat**, **depth**, and **table** characteristics. Hence $\\textbf{x} = [$ **carat**, **depth**, **table** $]$ for a given diamond.\n",
    "\n",
    "We are interested in using a linear model with a bias term as our model. We could express the model mathematically as:\n",
    "\n",
    "$$\n",
    "f_\\theta(\\textbf{x}) = f_\\theta\\left(\\textbf{carat}, \\textbf{depth}, \\textbf{table}\\right)\n",
    "=\n",
    "\\theta_0 + \n",
    "\\theta_1 * \\textbf{carat} +\n",
    "\\theta_2 * \\textbf{depth} +\n",
    "\\theta_3 * \\textbf{table}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1a\n",
    "Set the variable `data1` to be a subset of the original dataframe `data` such that `data1` only contains the columns `carat`, `depth`, `table` and `price`. (Note that the order of the columns in dataframe `data1` should follow the order `carat`, `depth`, `table`, `price` in order to pass the autograder test.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ...\n",
    "data1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we split `data1` into two variables:\n",
    "\n",
    "(1) Target values `y`: this consists of the prices of the diamonds.\n",
    "\n",
    "(2) Set of features `X_features`: this is a data frame where each row is a feacture vector consisting of features $[$ **carat**, **depth**, **table** $]$ (without the bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data1['price']\n",
    "X_features = data1[['carat', 'depth', 'table']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1b \n",
    "We defined a function `add_bias` which takes in a dataframe and adds a column of 1's to the left of the input dataframe. This function should modify the input dataframe in place. Please fill in this function with your solution. **Please name this extra column 'ones' in the dataframe.** After calling the function on `X_features` you will get a dataframe whose first five rows of `X_features` will look like the following:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ones</th>\n",
    "      <th>carat</th>\n",
    "      <th>depth</th>\n",
    "      <th>table</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>61.5</td>\n",
    "      <td>55.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.21</td>\n",
    "      <td>59.8</td>\n",
    "      <td>61.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>56.9</td>\n",
    "      <td>65.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.29</td>\n",
    "      <td>62.4</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.31</td>\n",
    "      <td>63.3</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Hint: You might find `pd.insert` method to be useful.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(data):\n",
    "    ...\n",
    "    \n",
    "X = X_features.copy() \n",
    "add_bias(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1c ###\n",
    "We need a loss function to evaluate how good our model approximates the prices of the diamonds. In the cell below, complete the function `avg_squared_loss` which returns the average squared loss between true target values `y` and our predictions `y_hat`. Note that both inputs, `y` and `y_hat`, to the function are arrays. You can assume that they have the same length. \n",
    "\n",
    "Recall that the average squared loss is defined as:\n",
    "$$\n",
    "Avg\\ Squared\\ Loss(y, \\hat{y}) = \\frac{1}{n} \\sum\\limits_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_squared_loss(y, y_hat):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build our linear model. In lecture, we saw that the **predictions** for the entire data set, $\\hat{\\mathbb{Y}}$, with a linear model can be computed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta  \n",
    "$$\n",
    "\n",
    "The **covariate matrix** $\\mathbb{X} \\in \\mathbb{R}^{n \\times (d+1)}$ consists of $n$ rows where each row corresponds to a record in the dataset and the $d+1$ columns correspond to the $d$ features extracted from the data plus an additional bias term.\n",
    "\n",
    "The following function `linear_model` computes the prediction $\\hat{\\mathbb{Y}}$ given parameters $\\theta$ and covariate matrix $\\mathbb{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(theta, X): \n",
    "    return X @ theta # The @ symbol is matrix multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `@` symbol is the matrix multiply operation and is equivalent to writing `X.dot(theta)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1d\n",
    "In the cell below, choose any `theta` you would like (please note that the dimension of the `theta` you choose should match the number of columns of the covariate matrix) and make predictions for `Y` using the linear model defined above given the `theta` you chose. Assign the variable `Y_hat` with the predictions and the variable `loss` with the average squared loss of your predictions based on the `theta` you chose.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = ...\n",
    "Y_hat = ...\n",
    "loss = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice the loss of the predictions for an arbitrary choice of `theta` is quite big. Recall from lecture, we can find the optimal `theta` by minimizing the mean square loss:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\frac{1}{n}\\sum_{i=1}^n \\left( \\mathbb{Y}_i - \\left(\\mathbb{X} \\theta\\right)_i \\right)^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n \\left( \\mathbb{Y}_i - \\mathbb{X}_i \\theta \\right)^2 \\\\\n",
    "&= \\frac{1}{n} || \\mathbb{Y} - \\mathbb{X} \\theta ||_2^2 \\\\\n",
    "&= \\frac{1}{n}\\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)^T \\left( \\mathbb{Y} - \\mathbb{X}\\theta \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video from Lecture 15 we saw that the normal equation (which we derived from a geometric perspective) is:\n",
    "\n",
    "$$\n",
    "(\\mathbb{X}^T \\mathbb{X}) \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n",
    "\n",
    "Solving for $\\hat{\\theta}$ in the above equation gives us the minimizer of the squared loss with respect to our data.  \n",
    "\n",
    "If $\\mathbb{X}^T \\mathbb{X}$ is invertible (full rank), $\\hat{\\theta}$ can be computed analytically as:\n",
    "\n",
    "$$\n",
    " \\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use the above analytic approach for solving $\\hat{\\theta}$ in this lab. Instead, we will use the `sklearn` library introduced in Lecture 16 to fit our model to find the optimal $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression model from sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1e\n",
    "In the cell below, \n",
    "1. Fit a linear model using `X` and `Y` defined earlier in the lab. \n",
    "2. Make predictions `Y_hat` for `Y` using the fitted model.\n",
    "3. Calculate the average squared loss of your prediction.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "model.fit(X, Y)\n",
    "Y_hat = ...\n",
    "loss = ...\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we create a scatter plot by plotting (`Y`, `Y_pred`). The redline is the identity line where each point on the line has the same values for the variables representing the x-axis and y-axis. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_s = [1 for _ in range(len(Y))] # setting the sizes of the markers to 1\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.scatter(Y, Y_hat, s = Y_s)\n",
    "plt.plot(Y, Y, c = 'r') # the identity line\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Y_hat')\n",
    "plt.title('Y vs Y_hat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 1, we only used the quantitative features `carat`, `depth`, `table`. As you can see from Question 1(e), the loss seems to be big. Is there a way to fit a better model by incorporating other features?\n",
    "\n",
    "In this second part of the lab, we explore incorporating qualitative features into our model.\n",
    "\n",
    "Recall our dataframe looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only incorporated information about `carat`, `depth`, `table` in our previous features. Do `cut`, `color`, and `clarity` matter when it comes to predicting the prices of the diamonds?\n",
    "\n",
    "Based on this online article https://www.pricescope.com/diamond-prices, these characteristics should matter! So let's try to incoporate these into our model!\n",
    "\n",
    "Recall from Lecture 16, to include qualitative variables as features, we may use one-hot encoding. The idea of one-hot encoding is to vectorize the variables with 1's and 0's. For example, suppose we have a qualitative variable `smoking` and the variable can take on either 'smoker' or 'non-smoker' like what we show below:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "          <th></th>\n",
    "      <th>smoking</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <th>smoker</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <th>non-smoker</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <th>smoker</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <th>non-smoker</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <th>non-smoker</th>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one-hot encoding, the resulting dataframe will look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "          <th></th>\n",
    "      <th>smoker</th>\n",
    "      <th>non-smoker</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "      <th>0</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <th>1</th>\n",
    "      <th>0</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will use the [`DictVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) method from `sklearn` package to implement one-hot encoding.\n",
    "\n",
    "Let's first examine how the model will behave if we include the `cut` feature. In the cell below, we created a new dataframe `X_char_w_cut` which adds one more column `cut` to the features defined in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_with_cut = data[['carat', 'cut', 'depth', 'table']]\n",
    "add_bias(X_features_with_cut)\n",
    "X_features_with_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2a \n",
    "In the cell below, complete the code so that `X` is the new feature matrix after one-hot encoding. Note that in this question, `X` should be a numpy array.\n",
    "\n",
    "**Hint: You might want to consult the notebook from lecture 16.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec_enc = DictVectorizer()\n",
    "vec_enc.fit(X_features_with_cut.to_dict(orient='records'))\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2b\n",
    "Now please fit a linear model using our new covariate matrix `X`. Compute the average squared loss of the predictions. Compare this loss with the loss you computed in Question 1(e) without using the `cut` feature. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ...\n",
    "model1.fit(X, Y)\n",
    "Y_hat1 = ...\n",
    "loss1 = ...\n",
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 < loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 - loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss became smaller by incorporating the `cut` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "Y_s = [1 for _ in range(len(Y))] # setting the sizes of the markers to 1\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.scatter(Y, Y_hat1, s = Y_s) # the identity line \n",
    "plt.plot(Y, Y, c = 'r')\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Y_hat1')\n",
    "plt.title('Y vs Y_hat1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot looks similar to the earlier one we have. Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2c\n",
    "In the cell below, we consider adding `color` and `clarity` as features. Please fill in the relevant code below to fit a model with the covariate matrix `X_features_with_cut_color_clarity`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_with_cut_color_clarity = data[['carat', 'cut', 'color', 'clarity', 'depth', 'table']] # Do not change this line\n",
    "add_bias(X_features_with_cut_color_clarity)\n",
    "\n",
    "vec_enc = ...\n",
    "...\n",
    "X = ...\n",
    "\n",
    "model2 = ...\n",
    "model2.fit(X, Y)\n",
    "Y_hat2 = ...\n",
    "loss2 = ...\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 < loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 - loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictions\n",
    "Y_s = [1 for _ in range(len(Y))] # setting the sizes of the markers to 1\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.scatter(Y, Y_hat2, s = Y_s) # the identity line x = y\n",
    "plt.plot(Y, Y, c = 'r')\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Y_hat2')\n",
    "plt.title('Y vs Y_hat2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by further incorporting `color` and `clarity` as features, the loss becomes smaller.\n",
    "\n",
    "For Question 2, we see that by incorporating qualitative features using one-hot encoding, we were able to fit a linear model that makes the loss lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (Optional)\n",
    "If you are interested, try coming up with more features to make the model perform even better! Some suggestions are: include a `log(carat)` feature with the logarithmic values of `carat` or the characteristics `x`, `y`, `z` in the feature set. Write your code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Start Here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed this assignment. Hope you enjoyed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "ok.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
